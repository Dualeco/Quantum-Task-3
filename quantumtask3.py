# -*- coding: utf-8 -*-
"""QuantumTask3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vIoIyB_AfeoDKADdDdmNNrN-mfsryAlp
"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('internship_train.csv')

df.info()

from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(df, test_size=0.3, random_state=87)

X, y = train_set.drop('target', axis=1), train_set['target']
X_test, y_test = test_set.drop('target', axis=1), test_set['target']

corr_matrix = df.corr()
corr_matrix['target']

"""# Linear correlation between `target` and each of the features appears to be very low"""

X.info()

for col in X.columns:
  plt.scatter(X[col][:1000], y[:1000], alpha=0.1)
  plt.xlabel(col)
  plt.ylabel('Target')
  plt.show()
  print(col)

for col in X.columns:
  X[col].hist()
  plt.show()
  print(col)

"""# Most features seem to have a random distribution, while features `6` and `8` seem to be the only two that have a meaningful relationship with `target`"""

import numpy as np
is_any_row_value_null = np.any(X_non_random.isnull(), axis=1)
np.any(is_any_row_value_null)

"""#This means that there are no null values in the dataset

# Feature `6` seems to be continuous, while `8` looks like a one-hot feature

#Feature Engineering:

#Since feature `6` shows a `y = x^2` relationship, we could transform it into a linear relationship by squaring `x`.
"""

from sklearn.base import BaseEstimator, TransformerMixin
class SquaredFeatureAdder(BaseEstimator, TransformerMixin):
  def __init__(self, feature):
    self.feature = feature
  def fit(self, X, y=None):
    return self
  def transform(self, X, y=None):
    squared = X[self.feature] ** 2
    return np.c_[X, squared]

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

num_pipeline = Pipeline([
    ('squarer', SquaredFeatureAdder(feature='6')),
    ('scaler', StandardScaler())
])
full_pipeline = ColumnTransformer([
    ('num', num_pipeline, ['6']),
    ('cat', 'passthrough', ['8'])
])

X_train_prepared = full_pipeline.fit_transform(X)
X_test_prepared = full_pipeline.fit_transform(X_test)

X_train_prepared.shape

from sklearn.model_selection import cross_val_score

def display_scores(scores):
    print("Scores: ", scores)
    print("Mean: ", scores.mean())
    print("Std: ", scores.std())

"""#Let's try out Lasso, Linear and RandomForest regression"""

from sklearn.linear_model import Lasso

lasso_reg = Lasso()
lasso_reg.fit(X_train_prepared, y)

from sklearn.model_selection import GridSearchCV
lasso_param_grid = [
    {'alpha': [0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.01]}
]
grid_search = GridSearchCV(lasso_reg, lasso_param_grid, cv=5, 
                           scoring='neg_mean_squared_error', return_train_score=True)

grid_search.fit(X_train_prepared, y)
grid_search.best_params_

lasso_score = np.sqrt(-grid_search.best_score_)

lasso_reg = grid_search.best_estimator_

lasso_cv_scores = np.sqrt(-cross_val_score(lasso_reg, X_test_prepared, y_test, 
                                  scoring='neg_mean_squared_error', 
                                  cv=5))
display_scores(lasso_cv_scores)

"""#Linear regression without regularization:"""

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_cv_scores = np.sqrt(-cross_val_score(lin_reg, X_test_prepared, y_test, 
                                  scoring='neg_mean_squared_error', 
                                  cv=5))

display_scores(lin_cv_scores)

from sklearn.ensemble import RandomForestRegressor

tree_reg = RandomForestRegressor()
tree_reg.fit(X_train_prepared, y)

tree_cv_scores = np.sqrt(-cross_val_score(tree_reg, X_test_prepared, y_test, 
                                  scoring='neg_mean_squared_error', 
                                  cv=5))

display_scores(tree_cv_scores)

y.describe()

"""# It can be seen from the above that the linear models perform better than RandomForest on this dataset

#Let's now predict for the actual test set

#The scores of the Lasso and Linear regressors that were trained only become different in the 5th digit after the comma. Even though Lasso regression performs slightly worse, I'll prefer it to have a slightly less biased model.
"""

X_final_test = pd.read_csv('internship_hidden_test.csv')

X_final_test_prepared = full_pipeline.fit_transform(X_final_test)

final_predictions = lasso_reg.predict(X_final_test_prepared)

final_predictions_df = pd.DataFrame(final_predictions, columns=['target'])
final_predictions_df.to_csv("internship_hidden_test_predictions.csv")